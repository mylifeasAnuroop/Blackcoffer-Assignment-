# -*- coding: utf-8 -*-
"""Assignment

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kuXAIvPLGZGihCl7ViwSpiqhEy_tiO0t
"""

# SUBMITTED BY ANUROOP ARYA
'''Get the assignment files from Google Drive link:
https://drive.google.com/drive/folders/1ltdsXAS_zaZ3hI-q9eze_QCzHciyYAJY?usp=sharing'''


import numpy as np
import re
import os
import pandas as pd
from nltk.tokenize import RegexpTokenizer, sent_tokenize
from urllib.request import urlopen
from bs4 import BeautifulSoup
from fake_useragent import UserAgent
import requests
import urllib.request, sys, time, requests

# File paths for stopwords and wordlists
stopWordsFile = '/content/StopWords_Generic.txt'
positiveWordsFile = '/content/positive-words.txt'
negativeWordsFile = '/content/negative-words.txt'

# Read input URLs from Excel file
input = pd.read_excel("/content/Input.xlsx")

# Function to get article names from URLs
def get_article_names(urls):
    titles = []
    for i in range(len(urls)):
        title = urls[i]
        # Extract the title from the URL
        title_clean = title[title.index("m/") + 2:-1].replace('-', ' ')
        titles.append(title_clean)
    return titles

# Extract URLs from input and get article names
urls = input["URL"]
urlsTitleDF = get_article_names(urls)

# Function to fetch article content from URL
def fetch_article(url):
    page = requests.get(url, headers={"User-Agent": "XY"})
    soup = BeautifulSoup(page.text, 'html.parser')

    # Get title
    title_element = soup.find("h1", attrs={'class': 'entry-title'})
    title = title_element.get_text() if title_element else "No title found"

    # Get article text
    text_element = soup.find(attrs={'class': 'td-post-content'})
    text = text_element.get_text() if text_element else "No content found"

    # Clean up the text
    lines = (line.strip() for line in text.splitlines())
    chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
    text = '\n'.join(chunk for chunk in chunks if chunk)

    return title, text

# Fetch articles from URLs
corps = []
for url in urls:
    title, text = fetch_article(url)
    corps.append(text)

# Create DataFrame with article titles and content
df = pd.DataFrame({'title': urlsTitleDF, 'corps': corps})

# Function to tokenize text and remove stopwords
def tokenizer(text):
    text = text.lower()
    tokenizer = RegexpTokenizer(r'\w+')
    tokens = tokenizer.tokenize(text)
    filtered_words = list(filter(lambda token: token not in stopWordList, tokens))
    return filtered_words

# Function to calculate positive word count
def positive_score(text):
    posword = 0
    tokenphrase = tokenizer(text)
    for word in tokenphrase:
        if word in positiveWordList:
            posword += 1
    return posword

# Function to calculate negative word count
def negative_score(text):
    negword = 0
    tokenphrase = tokenizer(text)
    for word in tokenphrase:
        if word in negativeWordList:
            negword += 1
    return negword

# Function to calculate polarity score
def polarity_score(positive_score, negative_score):
    return (positive_score - negative_score) / ((positive_score + negative_score) + 0.000001)

# Function to count total words
def total_word_count(text):
    tokens = tokenizer(text)
    return len(tokens)

# Function to calculate average sentence length
def average_sentence_length(text):
    word_count = len(tokenizer(text))
    sentence_count = len(sent_tokenize(text))
    if sentence_count > 0:
        avg_sentence_length = word_count / sentence_count
    return round(avg_sentence_length)

# Function to count complex words
def complex_word_count(text):
    tokens = tokenizer(text)
    complex_word = 0
    for word in tokens:
        vowels = 0
        if word.endswith(('es', 'ed')):
            pass
        else:
            for w in word:
                if w in 'aeiou':
                    vowels += 1
            if vowels > 2:
                complex_word += 1
    return complex_word

# Function to calculate percentage of complex words
def percentage_complex_word(text):
    tokens = tokenizer(text)
    complex_word = 0
    for word in tokens:
        vowels = 0
        if word.endswith(('es', 'ed')):
            pass
        else:
            for w in word:
                if w in 'aeiou':
                    vowels += 1
            if vowels > 2:
                complex_word += 1
    if len(tokens) != 0:
        complex_word_percentage = complex_word / len(tokens)
    return complex_word_percentage

# Function to calculate fog index
def fog_index(average_sentence_length, percentage_complex_word):
    return 0.4 * (average_sentence_length + percentage_complex_word)

# Apply functions to DataFrame columns
df["total word count"] = df["corps"].apply(total_word_count)
df["percentage_complex_word"] = df["corps"].apply(percentage_complex_word)
df["complex_word_count"] = df["corps"].apply(complex_word_count)
df["average_sentence_length"] = df["corps"].apply(average_sentence_length)
df["positive_score"] = df["corps"].apply(positive_score)
df["negative_score"] = df["corps"].apply(negative_score)
df["polarity_score"] = np.vectorize(polarity_score)(df['positive_score'], df['negative_score'])

# Drop 'corps' column from DataFrame
final = df.drop("corps", axis=1)

# Save final DataFrame to Excel file
final.to_excel('Output Data Structure.xlsx')